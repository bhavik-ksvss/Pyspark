{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "8t4gs7Avx6rO"
   },
   "outputs": [],
   "source": [
    "# innstall java\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# install spark (change the version number if needed)\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# unzip the spark file to the current folder\n",
    "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "# set your spark folder to your system path environment. \n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
    "\n",
    "\n",
    "# install findspark using pip\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MrPAGLaLgpx"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAp1sngeLdYZ"
   },
   "source": [
    "* SparkContext is a Spark entry point with configuration information for running a Spark Application. It is critical because it manages memory and IO operations as well as task execution across multiple clusters. Spark Context first appeared in Spark 1.0. \n",
    "\n",
    "* SparkSession is another Spark entry point that includes all of the features of SparkContext as well as support for DataFrame and Dataset APIs. This enables us to create DataFrames that are both more efficient and flexible than RDDs. This enables us to work with both structured and semi-structured data. \n",
    "\n",
    "* SparkSession also allows multiple sessions to run on the same spark context. SparkContext and SparkSession are important because they manage and optimize job execution across multiple clusters.\n",
    "\n",
    "* They also help in the creation of scalable pipelines. They also support a variety of data sources, such as HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DAYJsbLLj2R"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Fh9_Kh-eE8W-"
   },
   "outputs": [],
   "source": [
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74zNY0a4Gr_M",
    "outputId": "f045fd9a-5180-4314-e573-018621654699"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Michael', 'salary': 3000},\n",
       " {'name': 'Andy', 'salary': 4500},\n",
       " {'name': 'Justin', 'salary': 3500},\n",
       " {'name': 'Berta', 'salary': 4000}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"Colab\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "emp = sc.textFile(os.environ.get('SPARK_HOME') + '/examples/src/main/resources/employees.json')\n",
    "data = emp.map(lambda x:json.loads(x))\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTHRd0I6LqHA"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1OU3FDJGvm8",
    "outputId": "56a47887-d8d0-43d4-8cb4-513b02ab38f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hc = HiveContext(sc)\n",
    "\n",
    "data_df = hc.read.json(os.environ.get('SPARK_HOME') + '/examples/src/main/resources/employees.json')\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY2y4tOgLur0"
   },
   "source": [
    "## Question 3a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ZwR1oqxvI20z"
   },
   "outputs": [],
   "source": [
    "data_df.registerTempTable(\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1i6mS9m9JJzF",
    "outputId": "39823460-36ca-42d5-eba3-5965cd2bf42f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdlACdvVMrxU"
   },
   "source": [
    "## Question 3b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "i2YKpwd7JN8h"
   },
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"select * from employee order by 2 desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vpsVkj8JfHD",
    "outputId": "3f462460-9985-40d3-9671-3682f164d63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|   Andy|  4500|\n",
      "|  Berta|  4000|\n",
      "| Justin|  3500|\n",
      "|Michael|  3000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RrN7pFORqe4"
   },
   "source": [
    "## Question 4a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "QYygDl47JhB4"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "data_df = sc.textFile(os.environ.get('SPARK_HOME') + '/examples/src/main/resources/people.txt')\\\n",
    ".map(lambda line: list(csv.reader([line]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "o6eGrAJGJtf9"
   },
   "outputs": [],
   "source": [
    "rows = data_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5zGnfGIRukA"
   },
   "source": [
    "## Question 4b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0Afl8HIJv-c",
    "outputId": "ee54338b-14e2-4276-8c65-7d2911778902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael', ' 29']\n",
      "['Andy', ' 30']\n",
      "['Justin', ' 19']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(rows)):\n",
    "    print(rows[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Roqga4hR0x7"
   },
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F0gSaqHK7H_"
   },
   "source": [
    "* SQLContext is the SparkSQL gateway, while HiveContext is the Hive gateway. Hive is capable of storing plain text tables in column-oriented formats such as HDFS.\n",
    "\n",
    "* Advantages of HiveContext:It is extremely simple to import data from Hive into Spark Applications.It has a querying interface that is similar to SQL and thus simple to use. It can read and write a wide range of file formats.\n",
    "\n",
    "* SQLContext has the following advantages: It has a similar interface to SQL for querying data, making it simple to use; and * It supports a wide range of file formats.\n",
    "* The following are the disadvantages of HiveContext and SQLContex.It is slower than RDDs. It is inappropriate for unstructured data.\n",
    "\n",
    "* Benefits of RDD: It is extremely flexible and powerful.It supports a wide range of file formats.It provides control over the pipeline, which can be used to optimize performance. It is significantly faster than Hivecontext or SQLContext.\n",
    "\n",
    "* RDD has the following disadvantages:It is difficult to work with; It takes longer to optimize for performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eLOQACeZC82"
   },
   "source": [
    "## References\n",
    "* https://towardsdatascience.com/pyspark-on-google-colab-101-d31830b238be\n",
    "* https://spark.apache.org/docs/latest/api/python/getting_started/index.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
